# Строки, кодировки и Go

> **Тема достаточно сложная, поэтому:**
>
> - если где-то неправ, поправляйте
> - если есть вопросы, задавайте

## Биты и байты

На заре появления первых компьютеров и программ перед инженерами встала проблема представления привычных им букв, цифр и знаков в понятный компьютеру формат. Нужно было придумать, как запрограммировать компьютер так, чтобы он мог хранить, например, строку "Hello", ведь символы "h", "e", "l", "o" ему непонятны - это не на его языке. Да и вообще таким понятием как "символ" компьютер не владеет.

Язык компьютера - это биты.

**Бит** — это один разряд двоичного кода (двоичная цифра). Может принимать только два взаимоисключающих значения: «да» или «нет», «1» или «0», «включено» или «выключено».

Да, в самой своей сути все компьютеры оперируют только битами - единицами и нулями. Вся информация, с которой работает компьютер, в конечном итоге представлена в виде единиц и нулей. То есть никакими буквами или прочими символами мы напрямую с компьютером обмениваться не можем - он просто нас не поймёт.

Но если бит может содержать всего одно из двух значений - 1 или 0, как с его помощью общаться с компьютером? Это все равно что пытаться общаться с людьми, используя лишь две буквы. Тут на помощь приходят байты.

**Байт** — это совокупность бит, обрабатываемых компьютером одновременно. Если в качестве метафоры считать биты за буквы, то байты можно условно рассматривать в качестве слов. Байт состоит из восьми бит, каждый из которых содержит 0 или 1. Например:

- 00000001 (в десятичной системе счисления равно 1)
- 00000010 (в десятичной системе счисления равно 2)
- 00000011 (в десятичной системе счисления равно 3)
- и так далее, включая все уникальные перестановки этих восьми бит

В общей сложности есть 256 уникальных последовательностей бит в рамках байта. То есть можно лишь 256 способами уникально упорядочить восемь бит.

## ASCII

Итого мы имеем, что компьютер понимает биты и их объединения в байты. Это его язык. А языком инженеров, разрабатывавших первые компьютеры, был английский. И они хотели, чтобы компьютерные программы тоже могли обрабатывать английский язык, а не только нули и единицы. И было принято напрашивающееся решение: создать некий словарь между языком компьютера (битами и байтами) и языком инженеров (английским). Словарь должен был представлять таблицу, в которой каждому символу английского алфавита, каждой цифре и специальному символу сопоставлялось какое-то численное значение. Тогда компьютер смог бы сопоставлять непонятные ему символы с понятными ему численными значениями (каждое численное значение можно перевести в двоичный формат). И такой словарь был создан и назван ASCII.

**ASCII** (American Standard Code for Information Interchange) — стандарт кодирования букв латинского алфавита, цифр, некоторых специальных знаков и управляющих символов, принятый в 1963 году Американской ассоциацией стандартов как основной способ представления текстовых данных в ЭВМ.

Когда ASCII был создан, оставалось дело за малым - внедрять поддержку этого стандарта в компьютеры. Таким образом, постепенно разные системы смогли "общаться" друг с другом на одном языке, сопоставляя каждому ASCII-символу одно и то же численное значение. Благодаря этому, программа, написанная на одной системе, поддерживающей ASCII, могла корректно работать на другой системе, поддерживающей ASCII.

Но оставалась проблема - ASCII изначально был семибитной кодировкой (из восьми бит полезную нагрузку несли только семь, а восьмой использовался в служебных целях), а значит позволял закодировать в себя не более 128 символов (2^7). Таким образом, кроме латинского алфавита, цифр, знаков препинания и некоторых управляющих символов, в ASCII ничего не помещалось. И либо весь цифровой мир должен был использовать только латинский алфавит, либо нужно было изобретать более универсальное решение, которое бы позволило стандартизировать все остальные символы помимо латинских, а так же огромное количество эмодзи и прочих символов.

## Unicode

Тогда в 1991-м впервые был предложен новый стандарт кодирования символов под названием Unicode. Unicode был призван включить в себя символы всех алфавитов мира, всех эмодзи и прочих специальных знаков, сопоставляя каждому символу численное обозначение. При этом в целях обратной совместимости с ASCII первые 128 позиций полностью с ним совпадали.

Стандарт состоит из двух основных частей: универсального набора символов (англ. Universal character set, UCS) и семейства кодировок (англ. Unicode transformation format, UTF). Универсальный набор символов перечисляет допустимые по стандарту Юникод символы и присваивает каждому символу код в виде неотрицательного целого числа, записываемого обычно в шестнадцатеричной форме с префиксом U+, например, U+040F. Семейство кодировок определяет способы преобразования кодов символов в двоичный вид для передачи в потоке или в файле.

Всего консорциумом было решено использовать 1 112 064 кодовых точек для совместимости всех способов представления Unicode (UTF). На данный момент под различные символы уже зарезервировано (то есть используется) около 160 000 кодовых точек.

Наибольшую популярность сыскал стандарт кодирования UTF-8, позволяющий компактно хранить и передавать символы Unicode, используя от одного до четырех байт в зависимости от численного номера символа в соответствии с Unicode. Например, представление символов латинского алфавита, расположенных в самом начале Unicode и имеющих численные коды меньше 128, умещается в одном байте, а вот эмодзи, кириллица или иероглифы требуют от одного до четырех байтов, поскольку их номер в Unicode имеет значение, превышающее один байт в двоичном эквиваленте.

Есть ещё, к примеру, UTF-32, который подразумевает выделение на каждый символ 32 бит (4 байта) вне зависимости от его численного значения по Unicode. Этот формат не оптимален с точки зрения использования памяти, но при этом реализует более простую логику преобразования символов, что иногда используется разработчиками оперативной памяти с целью повышения скорости обработки операций.

## Go и Unicode

Согласно спецификации Go исходный код программ на этом языке всегда должен записываться в кодировке UTF-8. Ряд функций из пакетов стандартной библиотеки рассчитывают на то, что переданная ими строка будет в формате UTF-8. То же можно сказать и о цикле for-range при итерировании по строке.

А вот так можно вывести кодовые точки Unicode для всех символов строки:

```go
fmt.Printf("%+q\n", "Хай") // "\u0425\u0430\u0439"
```

## Go и тип данных string

#### Строка - это массив байтов

Под капотом у Go для представления строковых значений на самом деле используются массивы байтов, то есть `[]byte`. Рассмотрим это на примере.

Объявим функцию `stringStat(str string)`, которая принимает в качестве аргумента строку и выводит её значение, длину, полученную функцией `len()`, и массив байтов, представляющий эту строку.

```go
func stringStat(str string) {
    fmt.Printf("Строка: %s\n", str)
    fmt.Printf("Длина: %d\n", len(str))
    fmt.Printf("Байты: %+v\n", []byte(str))
}
```

Вызовем эту функцию для строки `"Hi"`, состоящей из двух букв латинского алфавита:

```go
stringStat("Hi")
```

Вывод в терминале будет следующим:

```
Строка: Hi
Длина: 2
Байты: [72 105]
```

Здесь мы видим, что длина строки равняется двум, как мы и ожидали, ведь в строке два символа. Помимо этого мы видим массив байтов, состоящий из двух элементов, что также количественно соответствует длине строки с нашей точки зрения. `72` и `105` - это численные значения, полученные в результате работы алгоритмов UTF-8 по кодированию символов `"H"` и `"i"`.

Теперь вызовем эту же функцию для строки с кириллицей, состоящей из трёх букв русского алфавита:

```go
stringStat("Хай")
```

Вывод в терминале будет следующим:

```
Строка: Хай
Длина: 6
Байты: [208 165 208 176 208 185]
```

Первое, что можно заметить, это значение длины, равное 6. В переданном слове три буквы, но функция `len()` вернула значение 6.

Дело в том, что стандартная функция `len()` для подсчёта длины строки в Go всегда возвращает именно количество байтов, представляющих эту строку, то есть длину того самого массива байтов, который располагается в памяти вместо строки непосредственно.

В данном случае результатом работы алгоритмов UTF-8 по кодированию символов `"Х"`, `"а"`, `"й"` станет представленный выше массив байтов. Символ `"Х"` будет закодирован в два первых байта, `"а"` в третий и четвертый, а `"й"` - в пятый и шестой соответственно. Здесь наглядно видно, что алгоритмы UTF-8 выделяют разное количество байтов в зависимости от символа, и это количество байтов не совпадает с длиной слова в том смысле, в котором мы привыкли о ней думать. Некоторые китайские иероглифы после кодирования в UTF-8 занимают целых четыре байта. Многие эмодзи тоже имеют представляются более чем одним байтом.

### todo

- строка неизменяема посимвольно
- Операции среза строк в Go особенно эффективны, потому что новые строки создаются путем указания на тот же массив байтов, что и исходная строка, с изменением только начальной позиции и длины. Однако, если изначально была очень длинная строка, от которой создали короткую, а на длинную перестали ссылаться в коде, весь огромный массив байтов по исходной строке продолжит лежать в памяти - GC его не соберет, поскольку короткая строка на него все еще ссылается. с пометкой "по идее"

- длина строки - что на самом деле означает и чему можно удивиться, utf8.RuneCountInString(str)

- построение строки через += и через буфер и через strings.Builder

- итерация по строке, в том числе включающей кириллицу
- индексы при итерации по строке
- руны
- дороговизна операций сравнения строк в Go, дороговизна их использования в качестве ключей в map

## Полезные ссылки

- [Understanding Unicode and UTF-8 with Golang](https://dev.to/mohamadharith/understanding-unicode-and-utf-8-with-golang-ojo)
-
